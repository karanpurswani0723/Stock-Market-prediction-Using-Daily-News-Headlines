{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):\n",
    "    \n",
    "    analysis = TextBlob((str(tweet)))     #defining the function which will find the plority of a sentence\n",
    "    return analysis.polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TABLE OF FREQUENCY WORD DISTRIBUTION (1492, 4733)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "news=pd.read_csv('/Users/karanpurswani/Documents/practicals/Stock-Market-prediction-Using-Daily-News-Headlines/Combined_News_DJIA.csv')\n",
    "\n",
    "train_news = news[news['Date'] < '2014-07-15']   # SPLITTING THE DATASET INTO TRAINING AND TESTING\n",
    "test_news = news[news['Date'] > '2014-07-14']\n",
    "\n",
    "train_news_list = []\n",
    "for row in range(0,len(train_news.index)): # CONVERT THE TRAINNG DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,2:27]))\n",
    "    \n",
    "vectorize= CountVectorizer(min_df=0.01, max_df=0.8) # DEFINING THE VECTOR FUNCTION, SPECIFYING THR MIN AND MAX WORD FREQUENCY FILTER\n",
    "news_vector = vectorize.fit_transform(train_news_list) # TRANSFORMING THE TRAINING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "print( \"THE TABLE OF FREQUENCY WORD DISTRIBUTION\" , news_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline model accuracy 0.4607645875251509\n",
      "Top ten words according to the baseline model           Word  Coefficient\n",
      "3728      self     0.628725\n",
      "4647      wing     0.533285\n",
      "2090  hospital     0.533244\n",
      "2392     kills     0.528196\n",
      "4387      turn     0.518748\n",
      "284      among     0.516599\n",
      "762     cartel     0.514192\n",
      "2929  olympics     0.508684\n",
      "1146  damascus     0.508380\n",
      "3585      rise     0.504063\n",
      "Last ten words according to the baseline model            Word  Coefficient\n",
      "3770        sex    -0.538371\n",
      "1163         de    -0.542079\n",
      "990       congo    -0.545630\n",
      "4206     terror    -0.554574\n",
      "4047   students    -0.562809\n",
      "3653  sanctions    -0.570902\n",
      "2100      hours    -0.571660\n",
      "506       begin    -0.603627\n",
      "4301      total    -0.610774\n",
      "3626        run    -0.663518\n",
      " TFID TRANSFOMATION DATAFRAME SHAPE (1492, 284)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression()\n",
    "model = lr.fit(news_vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))# CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "\n",
    "test_vector = vectorize.transform(test_news_list) # TRANSFORMING THE TESTING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "\n",
    "predictions = model.predict(test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy1=accuracy_score(test_news['Label'], predictions)\n",
    "print(\"the baseline model accuracy\", accuracy1)\n",
    "\n",
    "words = vectorize.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : words,'Coefficient' : coefficients})  # WORD DISTRIBUTION OF THE MODEL\n",
    "\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"Top ten words according to the baseline model\",coeffdf.head(10))\n",
    "print(\"Last ten words according to the baseline model\",coeffdf.tail(10))\n",
    "\n",
    "\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.85,ngram_range=(2,2)) # DEFINING THE TFID TRANSFORMATION FUNCTION\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "print(\" TFID TRANSFOMATION DATAFRAME SHAPE\",news_nvector.shape)\n",
    "\n",
    "nmodel = lr.fit(news_nvector, train_news[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistics Regression with Bigram and TFID 0.5311871227364185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with tfid and bigram 0.545271629778672\n"
     ]
    }
   ],
   "source": [
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "npredictions = nmodel.predict(ntest_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], npredictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy2=accuracy_score(test_news['Label'], npredictions)\n",
    "print(\" Logistics Regression with Bigram and TFID\",accuracy2)\n",
    "\n",
    "nwords = nvectorize.get_feature_names()\n",
    "ncoefficients = nmodel.coef_.tolist()[0]\n",
    "ncoeffdf = pd.DataFrame({'Word' : nwords, \n",
    "                        'Coefficient' : ncoefficients})\n",
    "ncoeffdf = ncoeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "ncoeffdf.head(10)\n",
    "ncoeffdf.tail(10)\n",
    "\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95,ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "rfmodel = RandomForestClassifier(random_state=55)  #DEFINNG THE RANDOM FOREST MODEL\n",
    "rfmodel = rfmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test_news['Label'], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy:  0.5291750503018109\n",
      " CONFUSION MATRIX OF THE GRADIANT BOOSTING  [[ 92 148]\n",
      " [ 74 183]]\n",
      "Gradient Boosting accuracy:  0.5533199195171026\n",
      "(1492, 569061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIGARAM ACCURACY 0.5171026156941649\n",
      "trigram top ten word distibution                       Word  Coefficient\n",
      "509383           to the us     0.201466\n",
      "481307        the right to     0.170945\n",
      "322285   nobel peace prize     0.166078\n",
      "223934  human rights watch     0.159698\n",
      "491158         this is the     0.154684\n",
      "240342        in west bank     0.151686\n",
      "230935        in china the     0.139410\n",
      "518984         turn out to     0.138018\n",
      "239146     in the occupied     0.132465\n",
      "321584         no fly zone     0.127306\n",
      "trigram last ten word distibution                          Word  Coefficient\n",
      "183898      freedom of speech    -0.141464\n",
      "356524        osama bin laden    -0.141908\n",
      "371338  phone hacking scandal    -0.147679\n",
      "497344              to be the    -0.148085\n",
      "207018      has been arrested    -0.151972\n",
      "509742              to try to    -0.152776\n",
      "334728        of human rights    -0.170488\n",
      "416292             said to be    -0.191689\n",
      "48303        around the world    -0.195347\n",
      "238814         in the country    -0.223679\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.8,ngram_range=(2,2)) #DEFINING THE NAIVE BAYS MODEL\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "nbmodel = MultinomialNB(alpha=0.5)\n",
    "nbmodel = nbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "nbpredictions = nbmodel.predict(ntest_vector)\n",
    "nbaccuracy=accuracy_score(test_news['Label'], nbpredictions)\n",
    "print(\"Naive Bayes accuracy: \",nbaccuracy)\n",
    "\n",
    "#author: Shravan Chintha\n",
    "#Gradient Boosting Classifier\n",
    "\n",
    "gbmodel = GradientBoostingClassifier(random_state=52)  # DEFINING THE GARDIANT BOOSTING MODEL\n",
    "gbmodel = gbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "gbpredictions = gbmodel.predict(ntest_vector.toarray())\n",
    "gbaccuracy = accuracy_score(test_news['Label'], gbpredictions)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\" CONFUSION MATRIX OF THE GRADIANT BOOSTING \", confusion_matrix(test_news['Label'], gbpredictions))\n",
    "\n",
    "\n",
    "print(\"Gradient Boosting accuracy: \",gbaccuracy)\n",
    "\n",
    "\n",
    "\n",
    "n3vectorize = TfidfVectorizer(min_df=0.0004, max_df=0.115,ngram_range=(3,3)) # DEFINING THE TFID , TRIGRAM MODEL\n",
    "news_n3vector = n3vectorize.fit_transform(train_news_list)\n",
    "\n",
    "print(news_n3vector.shape)\n",
    "\n",
    "n3model = lr.fit(news_n3vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "n3test_vector = n3vectorize.transform(test_news_list)\n",
    "n3predictions = n3model.predict(n3test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], n3predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy3=accuracy_score(test_news['Label'], n3predictions)\n",
    "print(\"TRIGARAM ACCURACY\", accuracy3)\n",
    "\n",
    "n3words = n3vectorize.get_feature_names()\n",
    "n3coefficients = n3model.coef_.tolist()[0]\n",
    "n3coeffdf = pd.DataFrame({'Word' : n3words, \n",
    "                        'Coefficient' : n3coefficients})\n",
    "n3coeffdf = n3coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"trigram top ten word distibution\", n3coeffdf.head(10))\n",
    "print(\"trigram last ten word distibution\", n3coeffdf.tail(10))    # trigram model word distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 75 165]\n",
      " [ 70 187]]\n",
      "Sentiment Accuracy 0.5271629778672032\n",
      "f1_score__ 0.5057056775644162\n"
     ]
    }
   ],
   "source": [
    "train_sentiment=train_news\n",
    "test_sentiment = test_news\n",
    "train_sentiment =train_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    train_sentiment[column]=train_sentiment[column].apply(analize_sentiment)  #converting the train headlines into polarity scores\n",
    "train_sentiment = train_sentiment+10  # removing negative co:efficient from the datset for better performance\n",
    "\n",
    "test_sentiment =test_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column]=test_sentiment[column].apply(analize_sentiment) # converting the test headlines into ploarity \n",
    "test_sentiment=test_sentiment+10 # removing negative co:efficient from the datset for better performance\n",
    "\n",
    "XGB_model= XGBClassifier()  # training the polarity score datset with DIJA \n",
    "gradiant=XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "y_pred= gradiant.predict(test_sentiment)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_news['Label'], y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Sentiment Accuracy\",accuracy_score(test_news['Label'], y_pred))\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"f1_score__\",f1_score(test_news['Label'], y_pred, average='weighted'))\n",
    "\n",
    "######################END####################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
