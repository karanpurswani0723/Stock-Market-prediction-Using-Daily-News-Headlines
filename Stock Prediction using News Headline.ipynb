{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):\n",
    "    \n",
    "    analysis = TextBlob((str(tweet)))     #defining the function which will find the plority of a sentence\n",
    "    return analysis.polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "news= pd.read_csv('/Users/adithyajob/Desktop/Courses/semester 3/AIT 590/project/Combined_News_DJIA.csv')\n",
    "\n",
    "train_news = news[news['Date'] < '2014-07-15']   # SPLITTING THE DATASET INTO TRAINING AND TESTING\n",
    "test_news = news[news['Date'] > '2014-07-14']\n",
    "\n",
    "train_news_list = []\n",
    "for row in range(0,len(train_news.index)): # CONVERT THE TRAINNG DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,2:27]))\n",
    "    \n",
    "vectorize= CountVectorizer(min_df=0.01, max_df=0.8) # DEFINING THE VECTOR FUNCTION, SPECIFYING THR MIN AND MAX WORD FREQUENCY FILTER\n",
    "news_vector = vectorize.fit_transform(train_news_list) # TRANSFORMING THE TRAINING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "print( \"THE TABLE OF FREQUENCY WORD DISTRIBUTION\" , news_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "model = lr.fit(news_vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))# CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "\n",
    "test_vector = vectorize.transform(test_news_list) # TRANSFORMING THE TESTING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "\n",
    "predictions = model.predict(test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy1=accuracy_score(test_news['Label'], predictions)\n",
    "print(\"the baseline model accuracy\", accuracy1)\n",
    "\n",
    "words = vectorize.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : words,'Coefficient' : coefficients})  # WORD DISTRIBUTION OF THE MODEL\n",
    "\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"Top ten words according to the baseline model\",coeffdf.head(10))\n",
    "print(\"Last ten words according to the baseline model\",coeffdf.tail(10))\n",
    "\n",
    "\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.85,ngram_range=(2,2)) # DEFINING THE TFID TRANSFORMATION FUNCTION\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "print(\" TFID TRANSFOMATION DATAFRAME SHAPE\",news_nvector.shape)\n",
    "\n",
    "nmodel = lr.fit(news_nvector, train_news[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "npredictions = nmodel.predict(ntest_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], npredictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy2=accuracy_score(test_news['Label'], npredictions)\n",
    "print(\" Logistics Regression with Bigram and TFID\",accuracy2)\n",
    "\n",
    "nwords = nvectorize.get_feature_names()\n",
    "ncoefficients = nmodel.coef_.tolist()[0]\n",
    "ncoeffdf = pd.DataFrame({'Word' : nwords, \n",
    "                        'Coefficient' : ncoefficients})\n",
    "ncoeffdf = ncoeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "ncoeffdf.head(10)\n",
    "ncoeffdf.tail(10)\n",
    "\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95,ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "rfmodel = RandomForestClassifier(random_state=55)  #DEFINNG THE RANDOM FOREST MODEL\n",
    "rfmodel = rfmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test_news['Label'], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.8,ngram_range=(2,2)) #DEFINING THE NAIVE BAYS MODEL\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "nbmodel = MultinomialNB(alpha=0.5)\n",
    "nbmodel = nbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "nbpredictions = nbmodel.predict(ntest_vector)\n",
    "nbaccuracy=accuracy_score(test_news['Label'], nbpredictions)\n",
    "print(\"Naive Bayes accuracy: \",nbaccuracy)\n",
    "\n",
    "#author: Shravan Chintha\n",
    "#Gradient Boosting Classifier\n",
    "\n",
    "gbmodel = GradientBoostingClassifier(random_state=52)  # DEFINING THE GARDIANT BOOSTING MODEL\n",
    "gbmodel = gbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "gbpredictions = gbmodel.predict(ntest_vector.toarray())\n",
    "gbaccuracy = accuracy_score(test_news['Label'], gbpredictions)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\" CONFUSION MATRIX OF THE GRADIANT BOOSTING \", confusion_matrix(test_news['Label'], gbpredictions))\n",
    "\n",
    "\n",
    "print(\"Gradient Boosting accuracy: \",gbaccuracy)\n",
    "\n",
    "\n",
    "\n",
    "n3vectorize = TfidfVectorizer(min_df=0.0004, max_df=0.115,ngram_range=(3,3)) # DEFINING THE TFID , TRIGRAM MODEL\n",
    "news_n3vector = n3vectorize.fit_transform(train_news_list)\n",
    "\n",
    "print(news_n3vector.shape)\n",
    "\n",
    "n3model = lr.fit(news_n3vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "n3test_vector = n3vectorize.transform(test_news_list)\n",
    "n3predictions = n3model.predict(n3test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], n3predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy3=accuracy_score(test_news['Label'], n3predictions)\n",
    "print(\"TRIGARAM ACCURACY\", accuracy3)\n",
    "\n",
    "n3words = n3vectorize.get_feature_names()\n",
    "n3coefficients = n3model.coef_.tolist()[0]\n",
    "n3coeffdf = pd.DataFrame({'Word' : n3words, \n",
    "                        'Coefficient' : n3coefficients})\n",
    "n3coeffdf = n3coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"trigram top ten word distibution\", n3coeffdf.head(10))\n",
    "print(\"trigram last ten word distibution\", n3coeffdf.tail(10))    # trigram model word distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment=train_news\n",
    "test_sentiment = test_news\n",
    "train_sentiment =train_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    train_sentiment[column]=train_sentiment[column].apply(analize_sentiment)  #converting the train headlines into polarity scores\n",
    "train_sentiment = train_sentiment+10  # removing negative co:efficient from the datset for better performance\n",
    "\n",
    "test_sentiment =test_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column]=test_sentiment[column].apply(analize_sentiment) # converting the test headlines into ploarity \n",
    "test_sentiment=test_sentiment+10 # removing negative co:efficient from the datset for better performance\n",
    "\n",
    "XGB_model= XGBClassifier()  # training the polarity score datset with DIJA \n",
    "gradiant=XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "y_pred= gradiant.predict(test_sentiment)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_news['Label'], y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Sentiment Accuracy\",accuracy_score(test_news['Label'], y_pred))\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"f1_score__\",f1_score(test_news['Label'], y_pred, average='weighted'))\n",
    "\n",
    "######################END####################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
